{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2a5ab4c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h1 align=\"center\">Information Retrival Systems</h1>\n",
    "    <h2 align=\"center\">Simple search engine project</h2>\n",
    "    <h3 align=\"center\">Phase 1</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61b31a7",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"./images/search2.jpg\" width=\"1000\" height=\"100\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c1d789",
   "metadata": {},
   "source": [
    "## Loading Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d0bd88",
   "metadata": {},
   "source": [
    "- We need Two important libraries for this project:\n",
    "- [ ] hazm\n",
    "- [ ] parsivar\n",
    "- I use parsivar for Normalization, Tokenization and Stemming and use hazm for find stopwords and remove them\n",
    "- Also we use pandas for working with dataframe and also numpy for working with arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aab0b925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsivar import Normalizer, Tokenizer, FindStems\n",
    "from hazm import stopwords_list\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from string import punctuation\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import string\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c613f3f",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1540f710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we work with google colab\n",
    "\"\"\"\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "data_path = '/content/drive/MyDrive/IR_data_news_12k.json'\n",
    "df = pd.read_json(data_path)\n",
    "\"\"\"\n",
    "# If we work on local system\n",
    "df = pd.read_json('Data/IR_data_news_12k.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89276015",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699f3073",
   "metadata": {},
   "source": [
    "- First We should get some information about our dataset. If we konw it well we can do better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38ee5e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>12192</th>\n",
       "      <th>12193</th>\n",
       "      <th>12194</th>\n",
       "      <th>12195</th>\n",
       "      <th>12196</th>\n",
       "      <th>12197</th>\n",
       "      <th>12198</th>\n",
       "      <th>12199</th>\n",
       "      <th>12200</th>\n",
       "      <th>12201</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <td>اعلام زمان قرعه کشی جام باشگاه های فوتسال آسیا</td>\n",
       "      <td>سجادی :حضور تماشاگران در  لیگ برتر فوتبال تابع...</td>\n",
       "      <td>محل برگزاری نشست‌های خبری سرخابی‌ها؛ مجیدی در ...</td>\n",
       "      <td>ماجدی در نشست با صالحی امیری: امیدوارم در این ...</td>\n",
       "      <td>لیگ‌برتر بسکتبال|‌ نخستین پیروزی شهرداری گرگان...</td>\n",
       "      <td>مسابقات تنیس روی میز فیدر قطر| هر4 بانوی ملی پ...</td>\n",
       "      <td>اعلام برنامه نشست خبری گل محمدی/ مجیدی هم باید...</td>\n",
       "      <td>احضار مدیران پرسپولیس به کمیته انضباطی  پیش از...</td>\n",
       "      <td>مدیر بی‌دستاورد، رئیس شد/ روزگار تلخ‌تر از تلخ...</td>\n",
       "      <td>خبر خوب برای استقلال؛ دانشگر با تیم تمرین کرد</td>\n",
       "      <td>...</td>\n",
       "      <td>کمیته ویژه‌ای از سوی رئیس جمهور مسئول پیگیری م...</td>\n",
       "      <td>مصری: با سفرهای استانی، کُلاه سر رئیس‌جمهور نم...</td>\n",
       "      <td>رئیسی به اردبیل سفر می‌کند/ توضیحاتی درباره مق...</td>\n",
       "      <td>رئیس دفتر رئیس جمهور وضعیت زلزله زرند را بررسی...</td>\n",
       "      <td>جزئیات گزارش سفر نظارتی اعضای کمیسیون فرهنگی م...</td>\n",
       "      <td>دفع حمله دزدان دریایی به دو نفتکش ایرانی توسط ...</td>\n",
       "      <td>نقدی بر یادداشت «مرزبندی گفتمانی با طالبان»/ و...</td>\n",
       "      <td>روند تحقیق و تفحص از سازمان سنجش با جدیت ادامه...</td>\n",
       "      <td>محسن اسلامی مدیرکل دفتر امور سیاسی وزارت کشور شد</td>\n",
       "      <td>جلسه سران قوا به‌ میزبانی رئیس مجلس برگزار شد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>content</th>\n",
       "      <td>\\nبه گزارش خبرگزاری فارس، کنفدراسیون فوتبال آس...</td>\n",
       "      <td>\\nبه گزارش خبرگزاری فارس، سید حمید سجادی در حا...</td>\n",
       "      <td>\\nبه گزارش خبرگزاری فارس، نشست خبری پیش از مسا...</td>\n",
       "      <td>\\nبه گزارش خبرگزاری فارس،  سید رضا صالحی امیری...</td>\n",
       "      <td>\\nبه گزارش خبرنگار ورزشی خبرگزاری فارس، در نخس...</td>\n",
       "      <td>\\nبه گزارش خبرگزاری فارس، چهار بانوی تنیس روی ...</td>\n",
       "      <td>\\nبه گزارش خبرگزاری فارس و به نقل از  سایت باش...</td>\n",
       "      <td>\\nبه گزارش خبرنگار ورزشی خبرگزاری فارس، کمیته ...</td>\n",
       "      <td>\\nبه گزارش خبرنگار ورزشی خبرگزاری فارس، مجمع ا...</td>\n",
       "      <td>\\nبه گزارش خبرنگار ورزشی خبرگزاری فارس، تمرین ...</td>\n",
       "      <td>...</td>\n",
       "      <td>\\nاسماعیلی حسین زهی نماینده مردم خاش در مجلس ش...</td>\n",
       "      <td>\\nعبدالرضا مصری نایب رئیس مجلس شورای اسلامی در...</td>\n",
       "      <td>\\nبه گزارش گروه سیاسی خبرگزاری فارس، سیّد صولت...</td>\n",
       "      <td>\\nبه گزارش گروه سیاسی خبرگزاری فارس غلامحسین ا...</td>\n",
       "      <td>\\nغلامرضا منتظری نماینده مردم گرگان در مجلس شو...</td>\n",
       "      <td>\\nبه گزارش گروه دفاعی خبرگزاری فارس، دریادار ش...</td>\n",
       "      <td>\\nخبرگزاری فارس ـ یادداشت میهمان ـ مسعود مرادی...</td>\n",
       "      <td>\\nبه گزارش خبرنگار پارلمانی خبرگزاری فارس، هیا...</td>\n",
       "      <td>\\nبه گزارش خبرگزاری فارس به نقل از پایگاه اطلا...</td>\n",
       "      <td>\\nبه گزارش خبرنگار پارلمانی خبرگزاری فارس، جلس...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tags</th>\n",
       "      <td>[اعلام زمان, قرعه‌کشی, قرعه‌کشی جام, قرعه‌کشی ...</td>\n",
       "      <td>[سجادی, لیگ, فدراسیون, وزیر ورزش]</td>\n",
       "      <td>[دربی 94, محل برگزاری, خبری سرخابی‌ها, مجیدی, ...</td>\n",
       "      <td>[کمیته امداد امام خمینی (ره), کمیته ملی المپیک...</td>\n",
       "      <td>[بسکتبال, لیگ برتر بسکتبال, شهرداری گرگان, تیم...</td>\n",
       "      <td>[مهشید اشتری, تنیس روی میز, ملی پوشان تنیس روی...</td>\n",
       "      <td>[باشگاه استقلال, یحیی گل محمدی, فرهاد مجیدی, ن...</td>\n",
       "      <td>[باشگاه پرسپولیس, احضار مدیران پرسپولیس به کمی...</td>\n",
       "      <td>[والیبال, پیمان رضایی, تیم والیبال شهرداری ارو...</td>\n",
       "      <td>[دانشگر با تیم تمرین کرد, محمد دانشگر, استقلال]</td>\n",
       "      <td>...</td>\n",
       "      <td>[مجلس شورای اسلامی, کمیسیون عمران, اسماعیل حسی...</td>\n",
       "      <td>[سید ابراهیم رئیسی, مجلس شورای اسلامی, رئیس‌جم...</td>\n",
       "      <td>[سید ابراهیم رئیسی, صولت مرتضوی, سفر رئیسی به ...</td>\n",
       "      <td>[غلامحسین اسماعیلی, رئیس دفتر رئیس جمهور, زلزل...</td>\n",
       "      <td>[مجلس شورای اسلامی, غلامرضا منتظری, نایب رئیس ...</td>\n",
       "      <td>[ارتش جمهوری اسلامی ایران, شهرام ایرانی, نیروی...</td>\n",
       "      <td>[طالبان, یادداشت, گفتمان, جمهوری اسلامی ایران,...</td>\n",
       "      <td>[مجلس شورای اسلامی, سازمان سنجش و آموزش کشور, ...</td>\n",
       "      <td>[وزارت کشور, احمد وحیدی, وزیر کشور, وزارت علوم...</td>\n",
       "      <td>[سران قوا, رئیس مجلس, رئیس جمهور, رئیس قوه قضا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>3/15/2022 5:59:27 PM</td>\n",
       "      <td>3/15/2022 5:30:07 PM</td>\n",
       "      <td>3/15/2022 5:20:01 PM</td>\n",
       "      <td>3/15/2022 5:18:00 PM</td>\n",
       "      <td>3/15/2022 5:16:41 PM</td>\n",
       "      <td>3/15/2022 5:15:34 PM</td>\n",
       "      <td>3/15/2022 3:57:47 PM</td>\n",
       "      <td>3/15/2022 3:43:18 PM</td>\n",
       "      <td>3/15/2022 3:35:21 PM</td>\n",
       "      <td>3/15/2022 3:21:04 PM</td>\n",
       "      <td>...</td>\n",
       "      <td>10/17/2021 7:17:00 AM</td>\n",
       "      <td>10/17/2021 6:05:00 AM</td>\n",
       "      <td>10/16/2021 8:58:18 PM</td>\n",
       "      <td>10/16/2021 8:47:20 PM</td>\n",
       "      <td>10/16/2021 7:38:13 PM</td>\n",
       "      <td>10/16/2021 6:25:36 PM</td>\n",
       "      <td>10/16/2021 5:03:39 PM</td>\n",
       "      <td>10/16/2021 4:54:47 PM</td>\n",
       "      <td>10/16/2021 4:14:30 PM</td>\n",
       "      <td>10/16/2021 4:05:31 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>url</th>\n",
       "      <td>https://www.farsnews.ir/news/14001224001005/اع...</td>\n",
       "      <td>https://www.farsnews.ir/news/14001224000982/سج...</td>\n",
       "      <td>https://www.farsnews.ir/news/14001224000971/مح...</td>\n",
       "      <td>https://www.farsnews.ir/news/14001224000964/ما...</td>\n",
       "      <td>https://www.farsnews.ir/news/14001224000947/لی...</td>\n",
       "      <td>https://www.farsnews.ir/news/14001224000916/مس...</td>\n",
       "      <td>https://www.farsnews.ir/news/14001224000865/اع...</td>\n",
       "      <td>https://www.farsnews.ir/news/14001224000842/اح...</td>\n",
       "      <td>https://www.farsnews.ir/news/14001224000811/مد...</td>\n",
       "      <td>https://www.farsnews.ir/news/14001224000819/خب...</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.farsnews.ir/news/14000724000586/کم...</td>\n",
       "      <td>https://www.farsnews.ir/news/14000724000679/مص...</td>\n",
       "      <td>https://www.farsnews.ir/news/14000724000905/رئ...</td>\n",
       "      <td>https://www.farsnews.ir/news/14000724000899/رئ...</td>\n",
       "      <td>https://www.farsnews.ir/news/14000724000826/جز...</td>\n",
       "      <td>https://www.farsnews.ir/news/14000724000790/دف...</td>\n",
       "      <td>https://www.farsnews.ir/news/14000724000611/نق...</td>\n",
       "      <td>https://www.farsnews.ir/news/14000724000694/رو...</td>\n",
       "      <td>https://www.farsnews.ir/news/14000724000651/مح...</td>\n",
       "      <td>https://www.farsnews.ir/news/14000724000639/جل...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports</td>\n",
       "      <td>...</td>\n",
       "      <td>politics</td>\n",
       "      <td>politics</td>\n",
       "      <td>politics</td>\n",
       "      <td>politics</td>\n",
       "      <td>politics</td>\n",
       "      <td>politics</td>\n",
       "      <td>politics</td>\n",
       "      <td>politics</td>\n",
       "      <td>politics</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 12202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0      \\\n",
       "title        اعلام زمان قرعه کشی جام باشگاه های فوتسال آسیا   \n",
       "content   \\nبه گزارش خبرگزاری فارس، کنفدراسیون فوتبال آس...   \n",
       "tags      [اعلام زمان, قرعه‌کشی, قرعه‌کشی جام, قرعه‌کشی ...   \n",
       "date                                   3/15/2022 5:59:27 PM   \n",
       "url       https://www.farsnews.ir/news/14001224001005/اع...   \n",
       "category                                             sports   \n",
       "\n",
       "                                                      1      \\\n",
       "title     سجادی :حضور تماشاگران در  لیگ برتر فوتبال تابع...   \n",
       "content   \\nبه گزارش خبرگزاری فارس، سید حمید سجادی در حا...   \n",
       "tags                      [سجادی, لیگ, فدراسیون, وزیر ورزش]   \n",
       "date                                   3/15/2022 5:30:07 PM   \n",
       "url       https://www.farsnews.ir/news/14001224000982/سج...   \n",
       "category                                             sports   \n",
       "\n",
       "                                                      2      \\\n",
       "title     محل برگزاری نشست‌های خبری سرخابی‌ها؛ مجیدی در ...   \n",
       "content   \\nبه گزارش خبرگزاری فارس، نشست خبری پیش از مسا...   \n",
       "tags      [دربی 94, محل برگزاری, خبری سرخابی‌ها, مجیدی, ...   \n",
       "date                                   3/15/2022 5:20:01 PM   \n",
       "url       https://www.farsnews.ir/news/14001224000971/مح...   \n",
       "category                                             sports   \n",
       "\n",
       "                                                      3      \\\n",
       "title     ماجدی در نشست با صالحی امیری: امیدوارم در این ...   \n",
       "content   \\nبه گزارش خبرگزاری فارس،  سید رضا صالحی امیری...   \n",
       "tags      [کمیته امداد امام خمینی (ره), کمیته ملی المپیک...   \n",
       "date                                   3/15/2022 5:18:00 PM   \n",
       "url       https://www.farsnews.ir/news/14001224000964/ما...   \n",
       "category                                             sports   \n",
       "\n",
       "                                                      4      \\\n",
       "title     لیگ‌برتر بسکتبال|‌ نخستین پیروزی شهرداری گرگان...   \n",
       "content   \\nبه گزارش خبرنگار ورزشی خبرگزاری فارس، در نخس...   \n",
       "tags      [بسکتبال, لیگ برتر بسکتبال, شهرداری گرگان, تیم...   \n",
       "date                                   3/15/2022 5:16:41 PM   \n",
       "url       https://www.farsnews.ir/news/14001224000947/لی...   \n",
       "category                                             sports   \n",
       "\n",
       "                                                      5      \\\n",
       "title     مسابقات تنیس روی میز فیدر قطر| هر4 بانوی ملی پ...   \n",
       "content   \\nبه گزارش خبرگزاری فارس، چهار بانوی تنیس روی ...   \n",
       "tags      [مهشید اشتری, تنیس روی میز, ملی پوشان تنیس روی...   \n",
       "date                                   3/15/2022 5:15:34 PM   \n",
       "url       https://www.farsnews.ir/news/14001224000916/مس...   \n",
       "category                                             sports   \n",
       "\n",
       "                                                      6      \\\n",
       "title     اعلام برنامه نشست خبری گل محمدی/ مجیدی هم باید...   \n",
       "content   \\nبه گزارش خبرگزاری فارس و به نقل از  سایت باش...   \n",
       "tags      [باشگاه استقلال, یحیی گل محمدی, فرهاد مجیدی, ن...   \n",
       "date                                   3/15/2022 3:57:47 PM   \n",
       "url       https://www.farsnews.ir/news/14001224000865/اع...   \n",
       "category                                             sports   \n",
       "\n",
       "                                                      7      \\\n",
       "title     احضار مدیران پرسپولیس به کمیته انضباطی  پیش از...   \n",
       "content   \\nبه گزارش خبرنگار ورزشی خبرگزاری فارس، کمیته ...   \n",
       "tags      [باشگاه پرسپولیس, احضار مدیران پرسپولیس به کمی...   \n",
       "date                                   3/15/2022 3:43:18 PM   \n",
       "url       https://www.farsnews.ir/news/14001224000842/اح...   \n",
       "category                                             sports   \n",
       "\n",
       "                                                      8      \\\n",
       "title     مدیر بی‌دستاورد، رئیس شد/ روزگار تلخ‌تر از تلخ...   \n",
       "content   \\nبه گزارش خبرنگار ورزشی خبرگزاری فارس، مجمع ا...   \n",
       "tags      [والیبال, پیمان رضایی, تیم والیبال شهرداری ارو...   \n",
       "date                                   3/15/2022 3:35:21 PM   \n",
       "url       https://www.farsnews.ir/news/14001224000811/مد...   \n",
       "category                                             sports   \n",
       "\n",
       "                                                      9      ...  \\\n",
       "title         خبر خوب برای استقلال؛ دانشگر با تیم تمرین کرد  ...   \n",
       "content   \\nبه گزارش خبرنگار ورزشی خبرگزاری فارس، تمرین ...  ...   \n",
       "tags        [دانشگر با تیم تمرین کرد, محمد دانشگر, استقلال]  ...   \n",
       "date                                   3/15/2022 3:21:04 PM  ...   \n",
       "url       https://www.farsnews.ir/news/14001224000819/خب...  ...   \n",
       "category                                             sports  ...   \n",
       "\n",
       "                                                      12192  \\\n",
       "title     کمیته ویژه‌ای از سوی رئیس جمهور مسئول پیگیری م...   \n",
       "content   \\nاسماعیلی حسین زهی نماینده مردم خاش در مجلس ش...   \n",
       "tags      [مجلس شورای اسلامی, کمیسیون عمران, اسماعیل حسی...   \n",
       "date                                  10/17/2021 7:17:00 AM   \n",
       "url       https://www.farsnews.ir/news/14000724000586/کم...   \n",
       "category                                           politics   \n",
       "\n",
       "                                                      12193  \\\n",
       "title     مصری: با سفرهای استانی، کُلاه سر رئیس‌جمهور نم...   \n",
       "content   \\nعبدالرضا مصری نایب رئیس مجلس شورای اسلامی در...   \n",
       "tags      [سید ابراهیم رئیسی, مجلس شورای اسلامی, رئیس‌جم...   \n",
       "date                                  10/17/2021 6:05:00 AM   \n",
       "url       https://www.farsnews.ir/news/14000724000679/مص...   \n",
       "category                                           politics   \n",
       "\n",
       "                                                      12194  \\\n",
       "title     رئیسی به اردبیل سفر می‌کند/ توضیحاتی درباره مق...   \n",
       "content   \\nبه گزارش گروه سیاسی خبرگزاری فارس، سیّد صولت...   \n",
       "tags      [سید ابراهیم رئیسی, صولت مرتضوی, سفر رئیسی به ...   \n",
       "date                                  10/16/2021 8:58:18 PM   \n",
       "url       https://www.farsnews.ir/news/14000724000905/رئ...   \n",
       "category                                           politics   \n",
       "\n",
       "                                                      12195  \\\n",
       "title     رئیس دفتر رئیس جمهور وضعیت زلزله زرند را بررسی...   \n",
       "content   \\nبه گزارش گروه سیاسی خبرگزاری فارس غلامحسین ا...   \n",
       "tags      [غلامحسین اسماعیلی, رئیس دفتر رئیس جمهور, زلزل...   \n",
       "date                                  10/16/2021 8:47:20 PM   \n",
       "url       https://www.farsnews.ir/news/14000724000899/رئ...   \n",
       "category                                           politics   \n",
       "\n",
       "                                                      12196  \\\n",
       "title     جزئیات گزارش سفر نظارتی اعضای کمیسیون فرهنگی م...   \n",
       "content   \\nغلامرضا منتظری نماینده مردم گرگان در مجلس شو...   \n",
       "tags      [مجلس شورای اسلامی, غلامرضا منتظری, نایب رئیس ...   \n",
       "date                                  10/16/2021 7:38:13 PM   \n",
       "url       https://www.farsnews.ir/news/14000724000826/جز...   \n",
       "category                                           politics   \n",
       "\n",
       "                                                      12197  \\\n",
       "title     دفع حمله دزدان دریایی به دو نفتکش ایرانی توسط ...   \n",
       "content   \\nبه گزارش گروه دفاعی خبرگزاری فارس، دریادار ش...   \n",
       "tags      [ارتش جمهوری اسلامی ایران, شهرام ایرانی, نیروی...   \n",
       "date                                  10/16/2021 6:25:36 PM   \n",
       "url       https://www.farsnews.ir/news/14000724000790/دف...   \n",
       "category                                           politics   \n",
       "\n",
       "                                                      12198  \\\n",
       "title     نقدی بر یادداشت «مرزبندی گفتمانی با طالبان»/ و...   \n",
       "content   \\nخبرگزاری فارس ـ یادداشت میهمان ـ مسعود مرادی...   \n",
       "tags      [طالبان, یادداشت, گفتمان, جمهوری اسلامی ایران,...   \n",
       "date                                  10/16/2021 5:03:39 PM   \n",
       "url       https://www.farsnews.ir/news/14000724000611/نق...   \n",
       "category                                           politics   \n",
       "\n",
       "                                                      12199  \\\n",
       "title     روند تحقیق و تفحص از سازمان سنجش با جدیت ادامه...   \n",
       "content   \\nبه گزارش خبرنگار پارلمانی خبرگزاری فارس، هیا...   \n",
       "tags      [مجلس شورای اسلامی, سازمان سنجش و آموزش کشور, ...   \n",
       "date                                  10/16/2021 4:54:47 PM   \n",
       "url       https://www.farsnews.ir/news/14000724000694/رو...   \n",
       "category                                           politics   \n",
       "\n",
       "                                                      12200  \\\n",
       "title      محسن اسلامی مدیرکل دفتر امور سیاسی وزارت کشور شد   \n",
       "content   \\nبه گزارش خبرگزاری فارس به نقل از پایگاه اطلا...   \n",
       "tags      [وزارت کشور, احمد وحیدی, وزیر کشور, وزارت علوم...   \n",
       "date                                  10/16/2021 4:14:30 PM   \n",
       "url       https://www.farsnews.ir/news/14000724000651/مح...   \n",
       "category                                           politics   \n",
       "\n",
       "                                                      12201  \n",
       "title         جلسه سران قوا به‌ میزبانی رئیس مجلس برگزار شد  \n",
       "content   \\nبه گزارش خبرنگار پارلمانی خبرگزاری فارس، جلس...  \n",
       "tags      [سران قوا, رئیس مجلس, رئیس جمهور, رئیس قوه قضا...  \n",
       "date                                  10/16/2021 4:05:31 PM  \n",
       "url       https://www.farsnews.ir/news/14000724000639/جل...  \n",
       "category                                           politics  \n",
       "\n",
       "[6 rows x 12202 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6908401",
   "metadata": {},
   "source": [
    "So it has 6 rows and 12202 columns. but this is unusual and we should transpose it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a6ffe89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>tags</th>\n",
       "      <th>date</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>اعلام زمان قرعه کشی جام باشگاه های فوتسال آسیا</td>\n",
       "      <td>\\nبه گزارش خبرگزاری فارس، کنفدراسیون فوتبال آس...</td>\n",
       "      <td>[اعلام زمان, قرعه‌کشی, قرعه‌کشی جام, قرعه‌کشی ...</td>\n",
       "      <td>3/15/2022 5:59:27 PM</td>\n",
       "      <td>https://www.farsnews.ir/news/14001224001005/اع...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>سجادی :حضور تماشاگران در  لیگ برتر فوتبال تابع...</td>\n",
       "      <td>\\nبه گزارش خبرگزاری فارس، سید حمید سجادی در حا...</td>\n",
       "      <td>[سجادی, لیگ, فدراسیون, وزیر ورزش]</td>\n",
       "      <td>3/15/2022 5:30:07 PM</td>\n",
       "      <td>https://www.farsnews.ir/news/14001224000982/سج...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>محل برگزاری نشست‌های خبری سرخابی‌ها؛ مجیدی در ...</td>\n",
       "      <td>\\nبه گزارش خبرگزاری فارس، نشست خبری پیش از مسا...</td>\n",
       "      <td>[دربی 94, محل برگزاری, خبری سرخابی‌ها, مجیدی, ...</td>\n",
       "      <td>3/15/2022 5:20:01 PM</td>\n",
       "      <td>https://www.farsnews.ir/news/14001224000971/مح...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ماجدی در نشست با صالحی امیری: امیدوارم در این ...</td>\n",
       "      <td>\\nبه گزارش خبرگزاری فارس،  سید رضا صالحی امیری...</td>\n",
       "      <td>[کمیته امداد امام خمینی (ره), کمیته ملی المپیک...</td>\n",
       "      <td>3/15/2022 5:18:00 PM</td>\n",
       "      <td>https://www.farsnews.ir/news/14001224000964/ما...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>لیگ‌برتر بسکتبال|‌ نخستین پیروزی شهرداری گرگان...</td>\n",
       "      <td>\\nبه گزارش خبرنگار ورزشی خبرگزاری فارس، در نخس...</td>\n",
       "      <td>[بسکتبال, لیگ برتر بسکتبال, شهرداری گرگان, تیم...</td>\n",
       "      <td>3/15/2022 5:16:41 PM</td>\n",
       "      <td>https://www.farsnews.ir/news/14001224000947/لی...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0     اعلام زمان قرعه کشی جام باشگاه های فوتسال آسیا   \n",
       "1  سجادی :حضور تماشاگران در  لیگ برتر فوتبال تابع...   \n",
       "2  محل برگزاری نشست‌های خبری سرخابی‌ها؛ مجیدی در ...   \n",
       "3  ماجدی در نشست با صالحی امیری: امیدوارم در این ...   \n",
       "4  لیگ‌برتر بسکتبال|‌ نخستین پیروزی شهرداری گرگان...   \n",
       "\n",
       "                                             content  \\\n",
       "0  \\nبه گزارش خبرگزاری فارس، کنفدراسیون فوتبال آس...   \n",
       "1  \\nبه گزارش خبرگزاری فارس، سید حمید سجادی در حا...   \n",
       "2  \\nبه گزارش خبرگزاری فارس، نشست خبری پیش از مسا...   \n",
       "3  \\nبه گزارش خبرگزاری فارس،  سید رضا صالحی امیری...   \n",
       "4  \\nبه گزارش خبرنگار ورزشی خبرگزاری فارس، در نخس...   \n",
       "\n",
       "                                                tags                  date  \\\n",
       "0  [اعلام زمان, قرعه‌کشی, قرعه‌کشی جام, قرعه‌کشی ...  3/15/2022 5:59:27 PM   \n",
       "1                  [سجادی, لیگ, فدراسیون, وزیر ورزش]  3/15/2022 5:30:07 PM   \n",
       "2  [دربی 94, محل برگزاری, خبری سرخابی‌ها, مجیدی, ...  3/15/2022 5:20:01 PM   \n",
       "3  [کمیته امداد امام خمینی (ره), کمیته ملی المپیک...  3/15/2022 5:18:00 PM   \n",
       "4  [بسکتبال, لیگ برتر بسکتبال, شهرداری گرگان, تیم...  3/15/2022 5:16:41 PM   \n",
       "\n",
       "                                                 url category  \n",
       "0  https://www.farsnews.ir/news/14001224001005/اع...   sports  \n",
       "1  https://www.farsnews.ir/news/14001224000982/سج...   sports  \n",
       "2  https://www.farsnews.ir/news/14001224000971/مح...   sports  \n",
       "3  https://www.farsnews.ir/news/14001224000964/ما...   sports  \n",
       "4  https://www.farsnews.ir/news/14001224000947/لی...   sports  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.transpose()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64d1b459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 12202 entries, 0 to 12201\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   title     12202 non-null  object\n",
      " 1   content   12202 non-null  object\n",
      " 2   tags      12202 non-null  object\n",
      " 3   date      12202 non-null  object\n",
      " 4   url       12202 non-null  object\n",
      " 5   category  12202 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 925.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a334b17",
   "metadata": {},
   "source": [
    "Thats very good. we dont have any null object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c88ab59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sports', 'politics'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c0de08",
   "metadata": {},
   "source": [
    "According to category column we only have two class of news : Sports and politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8685b498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12183"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['date'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47cea19",
   "metadata": {},
   "source": [
    "## Utils\n",
    "- Some functions that we use in next steps. we define them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94942bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_in_file(myinput, filename):\n",
    "    with open(filename, 'wb') as outp: \n",
    "        pickle.dump(myinput, outp, pickle.HIGHEST_PROTOCOL)\n",
    "        print(f'your input saved in {filename}')\n",
    "    \n",
    "def load_file(filename):\n",
    "    my_file = None\n",
    "    with open(filename, 'rb') as inp:\n",
    "        my_file = pickle.load(inp)\n",
    "    return my_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e9aac3",
   "metadata": {},
   "source": [
    "# Step1)\n",
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ac754e",
   "metadata": {},
   "source": [
    "- First we should define some functions for removing punctuations and stemming and removing stopwords. because for doing these preprocessing jobs we dont have a straightforward function in library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab3ead6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc (input_content):\n",
    "    return re.sub(r'[^\\w\\s]','',input_content)\n",
    "\n",
    "def stemmer (tokens):\n",
    "    stemmed = []\n",
    "    st = FindStems()\n",
    "    for i in tokens:\n",
    "        stemmed_token = st.convert_to_stem(i)\n",
    "        stemmed.append(stemmed_token)\n",
    "    return stemmed\n",
    "\n",
    "def stopwords_remover(stemmed):\n",
    "    stop_words = stopwords_list()\n",
    "    \n",
    "    for i in stemmed:\n",
    "        if i in stop_words:\n",
    "            stemmed.remove(i)\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96e4e33",
   "metadata": {},
   "source": [
    "- Now we define a function for single news preprocessing and showing each steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05064a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_preprocess (input_content):\n",
    "    print(\"Original content:\")\n",
    "    print(input_content+\"\\n\")\n",
    "    # 1) first we remove punctuations\n",
    "    punc_removed = remove_punc(input_content)\n",
    "    print(\"After removing puncs:\")\n",
    "    print(punc_removed+\"\\n\")\n",
    "    # 2) second we Normalize it\n",
    "    normal_content = Normalizer().normalize(punc_removed)\n",
    "    print(\"After normalizing:\")\n",
    "    print(normal_content+\"\\n\")\n",
    "    # 3) then we can tokenize content\n",
    "    tokened_content = Tokenizer().tokenize_words(normal_content)\n",
    "    print(\"After tokenizing:\")\n",
    "    print(tokened_content,'\\n')\n",
    "    # 4) then we remove stopwords\n",
    "    # (Attention: in the parsivar library there isn't any functions to remove stopwords. so we use hazm for this part)\n",
    "    removed_content = stopwords_remover(tokened_content)\n",
    "    print(\"After removing stopwords:\")\n",
    "    print(removed_content,\"\\n\")\n",
    "    # 5) then we stemming\n",
    "    stemmed_content = stemmer(removed_content)\n",
    "    print(\"After stemming:\")\n",
    "    print(stemmed_content,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfe81372",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original content:\n",
      "\n",
      "سعید احمدوند در گفت‌وکو با خبرنگار ورزشی خبرگزاری فارس، در مورد تغییر و تحولات در فدراسیون تنیس و برخی استعفاها در این فدراسیون اظهار داشت: این حق هر رئیس فدراسیون است که در مجموعه کاری خود تغییر و تحول بدهد و با هر کس که دوست دارد کار کند. در همه فدراسیون‌ها به این شکل است و در گذشته هم از این دست اتفاقات افتاده است. بارها دبیران فدراسیون تغییر کرده‌اند، اما به نظرم شکل اعتراضی که از سوی برخی صورت گرفت، می‌توانست منطقی‌تر باشد. از آن مهم‌تر اینکه وقتی اعتراض گروهی است،‌‌ بهتر است تا آخر همه پای آن بایستند نه اینکه وسط کار نظر خود را تغییر دهند. به هر حال رفتارهای انسانها با یکدیگر متفاوت است. در مجموع من با این شکل اعتراض موافق نبودم و بهتر بود تصمیم دیگری گرفته می‌شد. سرمربی سابق تیم ملی تنیس عنوان کرد: عزیزی رئیس فدراسیون بهتر است تغییر را در جاهایی که الزامی‌تر است صورت دهد، مثلاً ما در بخش آموزش واقعا ضعیف هستیم و هر چه سریع‌تر باید در مورد آن اقدام جدی صورت بگیرد. ما در سطح بسیار پائینی هستیم و اگر فکر می‌کنند صمت‌های آسیایی مسئول آموزش فدراسیون می‌تواند به این رشته کمک کند، باید گفت این سمت‌ها هم سال گذشته از دست رفت. وی ادامه داد: رئیس کمیته آموزش در همه جای فدراسیون هست و نمی‌دانم چه اصراری است که یک نفر تا این حد همه کارها را انجام دهد. وقتی به این شرایط اعتراض می‌کنم دلیلم این است که بازیکنی پیش من آمده که بک‌هند و فورهند را بلد نیست، این نشان می‌دهد که ما در آموزش ضعیف هستیم. به نظر من، رئیس فدراسیون باید توضیح دهد که چرا این فرد در همه بخش‌های تنیس دخالت می‌کند؟ استانی که او خودش تنیس بازی کرده یک بازیکن شاخصی ندارد. من دوست داشتم عزیزی در این بخش هم تغییراتی را ایجاد کند.     وی در مورد اینکه ظاهراً نامه استعفا قبل از رسیدن به دست رئیس فدراسیون در رسانه‌ها پخش شده است، گفت: این هم یک کار اشتباه بود. من چون خودم تجربه این کار را داشتم می‌گویم که بهتر بود ابتدا نامه به دست رئیس فدراسیون می‌رسید و بعداً اگر ترتیب اثری داده نمی‌شد، به رسانه‌ها می‌دادند. احمدوند در مورد تغییر سرمربی تیم ملی بانوان، گفت: فرناز فصیحی با اختلاف، بهترین مربی ایران است و هیچ‌کس به سطح او نمی‌تواند نزدیک هم بشود. او شاگردان زیادی را پرورش داده. به نظرم باید در مورد کنار گذاشتن او یک مقدار تجدیدنظر می‌شد. انتهای پیام/\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "After removing puncs:\n",
      "\n",
      "سعید احمدوند در گفتوکو با خبرنگار ورزشی خبرگزاری فارس در مورد تغییر و تحولات در فدراسیون تنیس و برخی استعفاها در این فدراسیون اظهار داشت این حق هر رئیس فدراسیون است که در مجموعه کاری خود تغییر و تحول بدهد و با هر کس که دوست دارد کار کند در همه فدراسیونها به این شکل است و در گذشته هم از این دست اتفاقات افتاده است بارها دبیران فدراسیون تغییر کردهاند اما به نظرم شکل اعتراضی که از سوی برخی صورت گرفت میتوانست منطقیتر باشد از آن مهمتر اینکه وقتی اعتراض گروهی است بهتر است تا آخر همه پای آن بایستند نه اینکه وسط کار نظر خود را تغییر دهند به هر حال رفتارهای انسانها با یکدیگر متفاوت است در مجموع من با این شکل اعتراض موافق نبودم و بهتر بود تصمیم دیگری گرفته میشد سرمربی سابق تیم ملی تنیس عنوان کرد عزیزی رئیس فدراسیون بهتر است تغییر را در جاهایی که الزامیتر است صورت دهد مثلا ما در بخش آموزش واقعا ضعیف هستیم و هر چه سریعتر باید در مورد آن اقدام جدی صورت بگیرد ما در سطح بسیار پائینی هستیم و اگر فکر میکنند صمتهای آسیایی مسئول آموزش فدراسیون میتواند به این رشته کمک کند باید گفت این سمتها هم سال گذشته از دست رفت وی ادامه داد رئیس کمیته آموزش در همه جای فدراسیون هست و نمیدانم چه اصراری است که یک نفر تا این حد همه کارها را انجام دهد وقتی به این شرایط اعتراض میکنم دلیلم این است که بازیکنی پیش من آمده که بکهند و فورهند را بلد نیست این نشان میدهد که ما در آموزش ضعیف هستیم به نظر من رئیس فدراسیون باید توضیح دهد که چرا این فرد در همه بخشهای تنیس دخالت میکند استانی که او خودش تنیس بازی کرده یک بازیکن شاخصی ندارد من دوست داشتم عزیزی در این بخش هم تغییراتی را ایجاد کند     وی در مورد اینکه ظاهرا نامه استعفا قبل از رسیدن به دست رئیس فدراسیون در رسانهها پخش شده است گفت این هم یک کار اشتباه بود من چون خودم تجربه این کار را داشتم میگویم که بهتر بود ابتدا نامه به دست رئیس فدراسیون میرسید و بعدا اگر ترتیب اثری داده نمیشد به رسانهها میدادند احمدوند در مورد تغییر سرمربی تیم ملی بانوان گفت فرناز فصیحی با اختلاف بهترین مربی ایران است و هیچکس به سطح او نمیتواند نزدیک هم بشود او شاگردان زیادی را پرورش داده به نظرم باید در مورد کنار گذاشتن او یک مقدار تجدیدنظر میشد انتهای پیام\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "After normalizing:\n",
      "سعید احمدوند در گفتوکو با خبرنگار ورزشی خبرگزاری فارس در مورد تغییر و تحولات در فدراسیون تنیس و برخی استعفاها در این فدراسیون اظهار داشت این حق هر رئیس فدراسیون است که در مجموعه کاری خود تغییر و تحول بدهد و با هر کس که دوست دارد کار کند در همه فدراسیونها به این شکل است و در گذشته هم از این دست اتفاقات افتاده است بارها دبیران فدراسیون تغییر کردهاند اما به نظرم شکل اعتراضی که از سوی برخی صورت گرفت میتوانست منطقیتر باشد از آن مهمتر اینکه وقتی اعتراض گروهی است بهتر است تا آخر همه پای آن بایستند نه اینکه وسط کار نظر خود را تغییر دهند به هر حال رفتارهای انسانها با یکدیگر متفاوت است در مجموع من با این شکل اعتراض موافق نبودم و بهتر‌بود تصمیم دیگری گرفته میشد سرمربی سابق تیم ملی تنیس عنوان کرد عزیزی رئیس فدراسیون بهتر است تغییر را در جاهایی که الزامیتر است صورت دهد مثلا ما در بخش آموزش واقعا ضعیف هستیم و هر چه سریعتر باید در مورد آن اقدام جدی صورت بگیرد ما در سطح بسیار پائینی هستیم و اگر فکر می‌کنند صمتهای آسیایی مسئول آموزش فدراسیون می‌تواند به این رشته کمک کند باید گفت این سمتها هم سال گذشته از دست رفت وی ادامه داد رئیس کمیته آموزش در همه جای فدراسیون هست و نمیدانم چه اصراری است که یک نفر تا این حد همه کارها را انجام دهد وقتی به این شرایط اعتراض میکنم دلیلم این است که بازیکنی پیش من آمده که بکهند و فورهند را بلد نیست این نشان می‌دهد که ما در آموزش ضعیف هستیم به نظر من رئیس فدراسیون باید توضیح دهد که چرا این فرد در همه بخشهای تنیس دخالت می‌کند استانی که او خودش تنیس بازی کرده یک بازیکن شاخصی ندارد من دوست داشتم عزیزی در این بخش هم تغییراتی را ایجاد کند    وی در مورد اینکه ظاهرا نامه استعفا قبل از رسیدن به دست رئیس فدراسیون در رسانهها پخش‌شده‌است گفت این هم یک کار اشتباه‌بود من چون خودم تجربه این کار را داشتم میگویم که بهتر‌بود ابتدا نامه به دست رئیس فدراسیون میرسید و بعدا اگر ترتیب اثری داده نمیشد به رسانهها می‌دادند احمدوند در مورد تغییر سرمربی تیم ملی بانوان گفت فرناز فصیحی با اختلاف بهترین مربی ایران است و هیچکس به سطح او نمیتواند نزدیک هم بشود او شاگردان زیادی را پرورش داده به نظرم باید در مورد کنار گذاشتن او یک مقدار تجدیدنظر میشد انتهای پیام\n",
      "\n",
      "After tokenizing:\n",
      "['سعید', 'احمدوند', 'در', 'گفتوکو', 'با', 'خبرنگار', 'ورزشی', 'خبرگزاری', 'فارس', 'در', 'مورد', 'تغییر', 'و', 'تحولات', 'در', 'فدراسیون', 'تنیس', 'و', 'برخی', 'استعفاها', 'در', 'این', 'فدراسیون', 'اظهار', 'داشت', 'این', 'حق', 'هر', 'رئیس', 'فدراسیون', 'است', 'که', 'در', 'مجموعه', 'کاری', 'خود', 'تغییر', 'و', 'تحول', 'بدهد', 'و', 'با', 'هر', 'کس', 'که', 'دوست', 'دارد', 'کار', 'کند', 'در', 'همه', 'فدراسیونها', 'به', 'این', 'شکل', 'است', 'و', 'در', 'گذشته', 'هم', 'از', 'این', 'دست', 'اتفاقات', 'افتاده', 'است', 'بارها', 'دبیران', 'فدراسیون', 'تغییر', 'کردهاند', 'اما', 'به', 'نظرم', 'شکل', 'اعتراضی', 'که', 'از', 'سوی', 'برخی', 'صورت', 'گرفت', 'میتوانست', 'منطقیتر', 'باشد', 'از', 'آن', 'مهمتر', 'اینکه', 'وقتی', 'اعتراض', 'گروهی', 'است', 'بهتر', 'است', 'تا', 'آخر', 'همه', 'پای', 'آن', 'بایستند', 'نه', 'اینکه', 'وسط', 'کار', 'نظر', 'خود', 'را', 'تغییر', 'دهند', 'به', 'هر', 'حال', 'رفتارهای', 'انسانها', 'با', 'یکدیگر', 'متفاوت', 'است', 'در', 'مجموع', 'من', 'با', 'این', 'شکل', 'اعتراض', 'موافق', 'نبودم', 'و', 'بهتر\\u200cبود', 'تصمیم', 'دیگری', 'گرفته', 'میشد', 'سرمربی', 'سابق', 'تیم', 'ملی', 'تنیس', 'عنوان', 'کرد', 'عزیزی', 'رئیس', 'فدراسیون', 'بهتر', 'است', 'تغییر', 'را', 'در', 'جاهایی', 'که', 'الزامیتر', 'است', 'صورت', 'دهد', 'مثلا', 'ما', 'در', 'بخش', 'آموزش', 'واقعا', 'ضعیف', 'هستیم', 'و', 'هر', 'چه', 'سریعتر', 'باید', 'در', 'مورد', 'آن', 'اقدام', 'جدی', 'صورت', 'بگیرد', 'ما', 'در', 'سطح', 'بسیار', 'پائینی', 'هستیم', 'و', 'اگر', 'فکر', 'می\\u200cکنند', 'صمتهای', 'آسیایی', 'مسئول', 'آموزش', 'فدراسیون', 'می\\u200cتواند', 'به', 'این', 'رشته', 'کمک', 'کند', 'باید', 'گفت', 'این', 'سمتها', 'هم', 'سال', 'گذشته', 'از', 'دست', 'رفت', 'وی', 'ادامه', 'داد', 'رئیس', 'کمیته', 'آموزش', 'در', 'همه', 'جای', 'فدراسیون', 'هست', 'و', 'نمیدانم', 'چه', 'اصراری', 'است', 'که', 'یک', 'نفر', 'تا', 'این', 'حد', 'همه', 'کارها', 'را', 'انجام', 'دهد', 'وقتی', 'به', 'این', 'شرایط', 'اعتراض', 'میکنم', 'دلیلم', 'این', 'است', 'که', 'بازیکنی', 'پیش', 'من', 'آمده', 'که', 'بکهند', 'و', 'فورهند', 'را', 'بلد', 'نیست', 'این', 'نشان', 'می\\u200cدهد', 'که', 'ما', 'در', 'آموزش', 'ضعیف', 'هستیم', 'به', 'نظر', 'من', 'رئیس', 'فدراسیون', 'باید', 'توضیح', 'دهد', 'که', 'چرا', 'این', 'فرد', 'در', 'همه', 'بخشهای', 'تنیس', 'دخالت', 'می\\u200cکند', 'استانی', 'که', 'او', 'خودش', 'تنیس', 'بازی', 'کرده', 'یک', 'بازیکن', 'شاخصی', 'ندارد', 'من', 'دوست', 'داشتم', 'عزیزی', 'در', 'این', 'بخش', 'هم', 'تغییراتی', 'را', 'ایجاد', 'کند', 'وی', 'در', 'مورد', 'اینکه', 'ظاهرا', 'نامه', 'استعفا', 'قبل', 'از', 'رسیدن', 'به', 'دست', 'رئیس', 'فدراسیون', 'در', 'رسانهها', 'پخش\\u200cشده\\u200cاست', 'گفت', 'این', 'هم', 'یک', 'کار', 'اشتباه\\u200cبود', 'من', 'چون', 'خودم', 'تجربه', 'این', 'کار', 'را', 'داشتم', 'میگویم', 'که', 'بهتر\\u200cبود', 'ابتدا', 'نامه', 'به', 'دست', 'رئیس', 'فدراسیون', 'میرسید', 'و', 'بعدا', 'اگر', 'ترتیب', 'اثری', 'داده', 'نمیشد', 'به', 'رسانهها', 'می\\u200cدادند', 'احمدوند', 'در', 'مورد', 'تغییر', 'سرمربی', 'تیم', 'ملی', 'بانوان', 'گفت', 'فرناز', 'فصیحی', 'با', 'اختلاف', 'بهترین', 'مربی', 'ایران', 'است', 'و', 'هیچکس', 'به', 'سطح', 'او', 'نمیتواند', 'نزدیک', 'هم', 'بشود', 'او', 'شاگردان', 'زیادی', 'را', 'پرورش', 'داده', 'به', 'نظرم', 'باید', 'در', 'مورد', 'کنار', 'گذاشتن', 'او', 'یک', 'مقدار', 'تجدیدنظر', 'میشد', 'انتهای', 'پیام'] \n",
      "\n",
      "After removing stopwords:\n",
      "['سعید', 'احمدوند', 'گفتوکو', 'خبرنگار', 'ورزشی', 'خبرگزاری', 'فارس', 'تحولات', 'فدراسیون', 'تنیس', 'برخی', 'استعفاها', 'فدراسیون', 'اظهار', 'حق', 'رئیس', 'فدراسیون', 'مجموعه', 'کاری', 'تحول', 'بدهد', 'کس', 'دوست', 'کار', 'فدراسیونها', 'شکل', 'گذشته', 'دست', 'اتفاقات', 'افتاده', 'بارها', 'دبیران', 'فدراسیون', 'کردهاند', 'نظرم', 'شکل', 'اعتراضی', 'از', 'برخی', 'صورت', 'میتوانست', 'منطقیتر', 'از', 'مهمتر', 'وقتی', 'اعتراض', 'آخر', 'پای', 'بایستند', 'وسط', 'کار', 'خود', 'حال', 'رفتارهای', 'انسانها', 'یکدیگر', 'مجموع', 'با', 'شکل', 'اعتراض', 'موافق', 'نبودم', 'بهتر\\u200cبود', 'تصمیم', 'گرفته', 'میشد', 'سرمربی', 'سابق', 'تیم', 'ملی', 'تنیس', 'عنوان', 'عزیزی', 'رئیس', 'فدراسیون', 'است', 'تغییر', 'جاهایی', 'الزامیتر', 'است', 'صورت', 'مثلا', 'آموزش', 'واقعا', 'ضعیف', 'سریعتر', 'آن', 'اقدام', 'صورت', 'بگیرد', 'سطح', 'پائینی', 'و', 'فکر', 'صمتهای', 'آسیایی', 'مسئول', 'آموزش', 'فدراسیون', 'این', 'رشته', 'کمک', 'این', 'سمتها', 'سال', 'گذشته', 'از', 'دست', 'وی', 'ادامه', 'رئیس', 'کمیته', 'آموزش', 'فدراسیون', 'هست', 'و', 'نمیدانم', 'چه', 'اصراری', 'است', 'نفر', 'این', 'همه', 'کارها', 'انجام', 'وقتی', 'این', 'شرایط', 'اعتراض', 'میکنم', 'دلیلم', 'این', 'است', 'بازیکنی', 'که', 'بکهند', 'و', 'فورهند', 'بلد', 'این', 'نشان', 'که', 'در', 'آموزش', 'ضعیف', 'به', 'من', 'رئیس', 'فدراسیون', 'توضیح', 'که', 'این', 'در', 'همه', 'بخشهای', 'تنیس', 'دخالت', 'استانی', 'که', 'تنیس', 'بازی', 'یک', 'بازیکن', 'شاخصی', 'من', 'دوست', 'داشتم', 'عزیزی', 'در', 'این', 'تغییراتی', 'ایجاد', 'وی', 'در', 'مورد', 'اینکه', 'ظاهرا', 'نامه', 'استعفا', 'از', 'به', 'دست', 'رئیس', 'فدراسیون', 'در', 'رسانهها', 'پخش\\u200cشده\\u200cاست', 'این', 'هم', 'یک', 'کار', 'اشتباه\\u200cبود', 'من', 'چون', 'خودم', 'تجربه', 'این', 'کار', 'را', 'داشتم', 'میگویم', 'که', 'بهتر\\u200cبود', 'نامه', 'به', 'دست', 'رئیس', 'فدراسیون', 'میرسید', 'و', 'بعدا', 'ترتیب', 'اثری', 'نمیشد', 'به', 'رسانهها', 'می\\u200cدادند', 'احمدوند', 'در', 'مورد', 'تغییر', 'سرمربی', 'تیم', 'ملی', 'بانوان', 'فرناز', 'فصیحی', 'با', 'اختلاف', 'مربی', 'ایران', 'است', 'و', 'هیچکس', 'به', 'سطح', 'نمیتواند', 'هم', 'بشود', 'شاگردان', 'را', 'پرورش', 'به', 'نظرم', 'باید', 'در', 'مورد', 'کنار', 'گذاشتن', 'او', 'یک', 'مقدار', 'تجدیدنظر', 'میشد', 'انتهای', 'پیام'] \n",
      "\n",
      "After stemming:\n",
      "['سعید', 'احمدوند', 'گفتوکو', 'خبرنگار', 'ورزشی', 'خبرگزاری', 'فارس', 'تحولات', 'فدراسیون', 'تنیس', 'برخی', 'استعفا', 'فدراسیون', 'اظهار', 'حق', 'رئیس', 'فدراسیون', 'مجموعه', 'کاری', 'تحول', 'داد&ده', 'کس', 'دوست', 'کار', 'فدراسیون', 'شکل', 'گذشته', 'دست', 'اتفاقا', 'افتاده', 'بار', 'دبیر', 'فدراسیون', 'کرد&کن', 'نظر', 'شکل', 'اعتراضی', 'از', 'برخی', 'صورت', 'توانست&توان', 'منطقی', 'از', 'مهم', 'وقتی', 'اعتراض', 'آخر', 'پای', 'ایستاد&ایست', 'وسط', 'کار', 'خود', 'حال', 'رفتار', 'انسان', 'یکدیگر', 'مجموع', 'با', 'شکل', 'اعتراض', 'موافق', 'بود&باش', 'بهتر\\u200cبود', 'تصمیم', 'گرفته', 'شد&شو', 'سرمربی', 'سابق', 'تیم', 'ملی', 'تنیس', 'عنوان', 'عزیزی', 'رئیس', 'فدراسیون', 'اس', 'تغییر', 'جا', 'الزامی', 'اس', 'صورت', 'مثلا', 'آموزش', 'واقعا', 'ضعیف', 'سریع', 'آن', 'اقدام', 'صورت', 'گرفت&گیر', 'سطح', 'پائین', 'و', 'فکر', 'صمتهای', 'آسیایی', 'مسئول', 'آموزش', 'فدراسیون', 'این', 'رشته', 'کمک', 'این', 'سمت', 'سال', 'گذشته', 'از', 'دست', 'وی', 'ادامه', 'رئیس', 'کمیته', 'آموزش', 'فدراسیون', 'هست', 'و', 'دانست&دان', 'چه', 'اصرار', 'اس', 'نفر', 'این', 'همه', 'کارها', 'انجام', 'وقتی', 'این', 'شرایط', 'اعتراض', 'کرد&کن', 'دلیل', 'این', 'اس', 'بازیکن', 'که', 'بکهند', 'و', 'فورهند', 'بلد', 'این', 'نشان', 'که', 'در', 'آموزش', 'ضعیف', 'به', 'من', 'رئیس', 'فدراسیون', 'توضیح', 'که', 'این', 'در', 'همه', 'بخش', 'تنیس', 'دخالت', 'استانی', 'که', 'تنیس', 'بازی', 'یک', 'بازیکن', 'شاخص', 'من', 'دوست', 'داشت&دار', 'عزیزی', 'در', 'این', 'تغییراتی', 'ایجاد', 'وی', 'در', 'مورد', 'اینکه', 'ظاهرا', 'نامه', 'استعفا', 'از', 'به', 'دست', 'رئیس', 'فدراسیون', 'در', 'رسانه', 'پخش\\u200cشده\\u200cاست', 'این', 'هم', 'یک', 'کار', 'اشتباه\\u200cبود', 'من', 'چون', 'خودم', 'تجربه', 'این', 'کار', 'را', 'داشت&دار', 'گفت&گو', 'که', 'بهتر\\u200cبود', 'نامه', 'به', 'دست', 'رئیس', 'فدراسیون', 'رسید&رس', 'و', 'بعدا', 'ترتیب', 'اثر', 'شد&شو', 'به', 'رسانه', 'داد&ده', 'احمدوند', 'در', 'مورد', 'تغییر', 'سرمربی', 'تیم', 'ملی', 'بانو', 'فرناز', 'فصیح', 'با', 'اختلاف', 'مربی', 'ایران', 'اس', 'و', 'هیچکس', 'به', 'سطح', 'توانست&توان', 'هم', 'بشود', 'شاگرد', 'را', 'پرورش', 'به', 'نظر', 'باید', 'در', 'مورد', 'کنار', 'گذاشتن', 'او', 'یک', 'مقدار', 'تجدیدنظر', 'شد&شو', 'انتهای', 'پیام'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "content_temp = df.iloc[27]['content']\n",
    "single_preprocess(content_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2768e971",
   "metadata": {},
   "source": [
    "- I define a function for do preprocessing step on all of data. It has three input arguments:  \n",
    "1- **input_content**: The input text like content column in dataset or query text  \n",
    "2- **is_stopwords_remove**: Its specify we can removing stop words or not ( we use in zipf law section)  \n",
    "3- **is_stemming**: Its specify we can do stemming words or not ( we use in heaps law section)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41d01bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_preprocess (input_content, is_stopwords_remove=True, is_stemming=True):\n",
    "  \n",
    "    # 1) first we remove punctuations\n",
    "    punc_removed = remove_punc(input_content)\n",
    "\n",
    "    # 2) second we Normalize it\n",
    "    normal_content = Normalizer().normalize(punc_removed)\n",
    "      \n",
    "    # 3) then we can tokenize content\n",
    "    updated_content = Tokenizer().tokenize_words(normal_content)\n",
    "      \n",
    "    # 4) then we remove stopwords\n",
    "    # (Attention: in the parsivar library there isn't any functions to remove stopwords. so we use hazm for this part)\n",
    "    if is_stopwords_remove:\n",
    "        updated_content = stopwords_remover(updated_content)\n",
    "      \n",
    "    # 5) then we stemming\n",
    "    if is_stemming:\n",
    "        updated_content = stemmer(updated_content)\n",
    "      \n",
    "    return updated_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a9ed57",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_content = []\n",
    "for i in range(len(df)):\n",
    "    if ( i % 2000 ==0):\n",
    "        print(i)\n",
    "    input_content = df.iloc[i]['content']\n",
    "    preprocessed_content.append(total_preprocess(input_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8877dd3a",
   "metadata": {},
   "source": [
    "- Because the last process is time consuming so we can save the preprocessed content in a file and when we restart the kernel of jupyter then we can load it from that file. for do it we can use the functions defined in Utils section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ede41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_in_file(preprocessed_content, 'preprocessed_content.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87719ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_content = load_file('preprocessed_content.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c90ad05",
   "metadata": {},
   "source": [
    "# Step2)\n",
    "## Creating Positional Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562976b4",
   "metadata": {},
   "source": [
    "- According to slides we need implementing dictionary and postings list for our inverted index. In python we can use dict datastructure for do this job. with using this accessing to each token and its postings list will become much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc632485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_index_creator(preprocessed_content):\n",
    "    \n",
    "    inverted_index = {}\n",
    "    \n",
    "    for doc_id in range(len(preprocessed_content)):\n",
    "        \n",
    "        # for checking progress\n",
    "        if doc_id%2000 == 0: print(doc_id)\n",
    "            \n",
    "        for token_pos in range(len(preprocessed_content[doc_id])):\n",
    "            \n",
    "            token = preprocessed_content[doc_id][token_pos]\n",
    "            \n",
    "            if token not in inverted_index:\n",
    "                inverted_index[token]={\n",
    "                    'doc_frequency': 1,\n",
    "                    'posting_list': [\n",
    "                        {\n",
    "                            doc_id:{\n",
    "                                'term_frequency': 1,\n",
    "                                'positions': [token_pos,]\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "                \n",
    "            else:\n",
    "                doc_id_list = []\n",
    "                for doc in inverted_index[token]['posting_list']:\n",
    "                    doc_id_list.append(list(doc.keys())[0])\n",
    "    \n",
    "                if doc_id in doc_id_list:\n",
    "            \n",
    "                    doc_index = doc_id_list.index(doc_id)\n",
    "                    inverted_index[token]['posting_list'][doc_index][doc_id]['term_frequency'] += 1\n",
    "                    inverted_index[token]['posting_list'][doc_index][doc_id]['positions'].append(token_pos)\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    inverted_index[token]['doc_frequency'] += 1\n",
    "                    inverted_index[token]['posting_list'].append({\n",
    "                        doc_id:{\n",
    "                            'term_frequency': 1,\n",
    "                            'positions': [token_pos,]\n",
    "                        }\n",
    "                    })\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12b1464",
   "metadata": {},
   "source": [
    "- Now we can give preprocessed data from step1 to above function for creating the inverted index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485e0ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = inverted_index_creator(preprocessed_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b502bdfb",
   "metadata": {},
   "source": [
    "- Because the last process is time consuming (like step1) so we can save the inverted index in a file and when we restart the kernel of jupyter then we can load it from that file. for do it we can use the functions defined in Utils section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a696048c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_in_file(inverted_index, 'inverted_index.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b3102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = load_file('inverted_index.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e8a836",
   "metadata": {},
   "source": [
    "- Also we can save dictionary part of our inverted index seperately in a variable to access it easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe7a2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = list(inverted_index.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9cffb6",
   "metadata": {},
   "source": [
    "# Step3&4)\n",
    "## Query Processing & Showing results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c867fe",
   "metadata": {},
   "source": [
    "- For doing this section I create a Query class. When we create a query object it should automatically find relevant contetns and return it. We can say this is a boolean query processing but we compute a rank for each doc and implementing ranked retrival. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63d214cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query():\n",
    "    def __init__(self, query_text):\n",
    "        self.query_text = query_text\n",
    "        # define some variables for seperating different part of query\n",
    "        self.not_tokens = self.get_not_token()\n",
    "        self.phrasal_tokens = self.get_phrasal_tokens()\n",
    "        self.normal = self.get_normals()\n",
    "        # then we should preprocess query like docs\n",
    "        if len(self.not_tokens)>0 : preprocessed_not_tokens = self.total_preprocess(\" \".join(self.not_tokens))\n",
    "        if len(self.normal)>0 : preprocessed_normals = self.total_preprocess(\" \".join(self.normal))\n",
    "        if len(self.phrasal_tokens)>0 : preprocessed_phrasal = [self.total_preprocess(phrase) for phrase in self.phrasal_tokens]\n",
    "        # we want to search among our dictionary words. so we should do this below job:\n",
    "        # (if we dont do this job its possible to ask from inverted index to return the postings\n",
    "        # of a word that not exists in our dictionary and so we get \"Key Error\")\n",
    "        self.final_not_tokens = None\n",
    "        self.final_normals = None\n",
    "        self.final_phrasal = None\n",
    "        if len(self.not_tokens)>0 : self.final_not_tokens = self.in_dictionary_checker(preprocessed_not_tokens)\n",
    "        if len(self.normal)>0 : self.final_normals = self.in_dictionary_checker(preprocessed_normals)\n",
    "        if len(self.phrasal_tokens)>0 : self.final_phrasal = self.in_dictionary_checker(preprocessed_phrasal, phrasal_mode=True)\n",
    "        \n",
    "        # We use below parameters in get_related_postings function for avoiding to get error\n",
    "        self.has_notToken = True\n",
    "        self.has_normal = True\n",
    "        self.has_phrasal = True\n",
    "        if self.final_not_tokens == None:\n",
    "            self.has_notToken = False\n",
    "        if self.final_normals == None:\n",
    "            self.has_normal = False\n",
    "        if self.final_phrasal == None:\n",
    "            self.has_phrasal = False\n",
    "\n",
    "        self.show_result()\n",
    "        \n",
    "    def get_not_token(self):\n",
    "        return re.findall(r'\\!\\s(\\w+)', self.query_text)\n",
    "    \n",
    "    def get_phrasal_tokens(self):\n",
    "        return re.findall(r'\"([^\"]*)\"', self.query_text)\n",
    "    \n",
    "    def get_normals(self):\n",
    "        normal_string = re.sub(r'\\!\\s\\w+', '', self.query_text)\n",
    "        normal_string = re.sub(r'\"[^\"]*\"', '', normal_string)\n",
    "        initial_normal = normal_string.split(' ')\n",
    "        normal = []\n",
    "        for token in initial_normal:\n",
    "            if len(token)>0:\n",
    "                normal.append(token)\n",
    "       \n",
    "        return normal\n",
    "    \n",
    "    def total_preprocess (self, input_content):\n",
    "        \n",
    "        # 1) first we remove punctuations\n",
    "        punc_removed = remove_punc(input_content)\n",
    "        \n",
    "        # 2) second we Normalize it\n",
    "        normal_content = Normalizer().normalize(punc_removed)\n",
    "\n",
    "        # 3) then we can tokenize content\n",
    "        tokened_content = Tokenizer().tokenize_words(normal_content)\n",
    "\n",
    "        # 4) then we remove stopwords\n",
    "        # (Attention: in the parsivar library there isn't any functions to remove stopwords. so we use hazm for this part)\n",
    "        deleted_stop_words = stopwords_remover(tokened_content)\n",
    "        \n",
    "        # 5) then we stemming\n",
    "        final_content = stemmer(deleted_stop_words)\n",
    "\n",
    "        return final_content \n",
    "    \n",
    "    def in_dictionary_checker(self,preprocessed_form, phrasal_mode=False):\n",
    "        final_tokens = []\n",
    "        # in phrasal mode if only we dont have one word of that phrase\n",
    "        # then we should remove it from our final phrases\n",
    "        if phrasal_mode:\n",
    "            for phrase in preprocessed_form:\n",
    "                flag = True\n",
    "                for token in phrase:\n",
    "                    if token not in dictionary:\n",
    "                        print(token)\n",
    "                        flag = False\n",
    "                if flag:\n",
    "                    final_tokens.append(phrase)\n",
    "        else:\n",
    "            for token in preprocessed_form:\n",
    "                if token in dictionary:\n",
    "                    final_tokens.append(token)\n",
    "        if len(final_tokens) == 0:\n",
    "            return None\n",
    "        return final_tokens\n",
    "    \n",
    "    # Step 4)\n",
    "    # Data set (in this step we should show the information that asked in project explanation)\n",
    "    def show_result(self):\n",
    "        final_postings = self.get_related_postings()\n",
    "        sorted_result = self.sorting(final_postings)\n",
    "        if len(sorted_result)==0:\n",
    "            print(\"There isn't any doc related to this query\")\n",
    "        else:\n",
    "            counter = 0\n",
    "            for result in sorted_result:\n",
    "                print(50*'/\\\\')\n",
    "                print(f'Rank: {result[1]}')\n",
    "                print(f'docID: {result[0]}')\n",
    "                print(f'Title: {df.loc[result[0]][\"title\"]}')\n",
    "                print(f'URL: {df.loc[result[0]][\"url\"]}')\n",
    "                print(f'{df.loc[result[0]][\"content\"]}')\n",
    "                print(50*'/\\\\')\n",
    "                counter += 1\n",
    "                if counter == 5:\n",
    "                    break\n",
    "\n",
    "    def sorting(self, final_postings):\n",
    "        result = {}\n",
    "        for posting in final_postings:\n",
    "            result[list(posting.keys())[0]] = posting[list(posting.keys())[0]]['term_frequency']\n",
    "        return sorted(result.items(), key=lambda x:x[1], reverse=True)\n",
    "        \n",
    "    def get_related_postings(self):\n",
    "        if self.has_normal:\n",
    "            #1) first we should calculate intersection between normal tokens' postings\n",
    "            postings1 = None\n",
    "            postings2 = None\n",
    "            for i in range(len(self.final_normals)):\n",
    "                if i==0:\n",
    "                    postings1= inverted_index[self.final_normals[i]]['posting_list']\n",
    "                    continue\n",
    "                postings2 = inverted_index[self.final_normals[i]]['posting_list']\n",
    "                postings1 = self.and_merge(postings1, postings2)\n",
    "            normals_intersect = postings1\n",
    "        #############################################################################\n",
    "        if self.has_phrasal:\n",
    "            #2)  also we should use positional merge between tokens in each phrase\n",
    "            phrase_intersect = []\n",
    "            for phrase in self.final_phrasal:\n",
    "                postings1 = None\n",
    "                postings2 = None\n",
    "                for i in range(len(phrase)):\n",
    "                    if i==0:\n",
    "                        postings1= inverted_index[phrase[i]]['posting_list']\n",
    "                        continue\n",
    "                    postings2 = inverted_index[phrase[i]]['posting_list']\n",
    "                    postings1 = self.positional_merge(postings1, postings2)\n",
    "                phrase_intersect.append(postings1)\n",
    "            \n",
    "            #3) Now we use the results of step2. now we can intersect between different phrases\n",
    "            # if(len(phrase_intersect)>1):\n",
    "            postings1 = None\n",
    "            postings2 = None\n",
    "            i = 0\n",
    "            for phrase_postings in phrase_intersect:\n",
    "                if i==0:\n",
    "                    postings1= phrase_postings\n",
    "                    continue\n",
    "                postings2 = phrase_postings\n",
    "                postings1 = self.and_merge(postings1, postings2)\n",
    "                i += 1\n",
    "            final_phrases_intersect = postings1\n",
    "            \n",
    "            if self.has_normal:\n",
    "                #4) the result of step1 and step3 are merged\n",
    "                final_and = self.and_merge(normals_intersect, final_phrases_intersect)\n",
    "            else:\n",
    "                final_and = final_phrases_intersect\n",
    "        elif self.has_normal:\n",
    "            final_and = normals_intersect\n",
    "        else:\n",
    "            final_and = []\n",
    "            return final_and\n",
    "        #############################################################################    \n",
    "        if self.has_notToken:\n",
    "            #5) union between the doc_id of postings list of tokens in not_tokens list\n",
    "            # according to and_not_merge algorithm we just need the doc_id of postings list and not tf or pose.\n",
    "            not_tokens_doc_ids = set()\n",
    "            for token in self.final_not_tokens:\n",
    "                for i in range(len(inverted_index[token]['posting_list'])):\n",
    "                    not_tokens_doc_ids.add(list(inverted_index[token]['posting_list'][i].keys())[0])\n",
    "            not_tokens_postings = sorted(list(not_tokens_doc_ids))\n",
    "            \n",
    "            #6) now we can use and_not_mege algorithm to arrive to final postings\n",
    "            return self.and_not_merge(final_and,not_tokens_postings)\n",
    "        else:\n",
    "            return final_and\n",
    "    \n",
    "    # We write this method and two others from psudo code in slides\n",
    "    def and_merge(self,postings1, postings2):\n",
    "        result = []\n",
    "        p1 = 0\n",
    "        p2 = 0\n",
    "        while p1 < len(postings1) and p2 < len(postings2):\n",
    "            \n",
    "            p1_doc_id = list(postings1[p1].keys())[0]\n",
    "            p2_doc_id = list(postings2[p2].keys())[0]\n",
    "            \n",
    "            if p1_doc_id == p2_doc_id:\n",
    "                result.append({p1_doc_id:{\n",
    "                    'term_frequency': postings1[p1][p1_doc_id]['term_frequency']+postings2[p2][p2_doc_id]['term_frequency'],\n",
    "                    'positions':postings1[p1][p1_doc_id]['positions']+postings2[p2][p2_doc_id]['positions']\n",
    "                }})\n",
    "                p1 += 1\n",
    "                p2 += 1\n",
    "            elif p1_doc_id < p2_doc_id:\n",
    "                p1 += 1\n",
    "            else:\n",
    "                p2 += 1\n",
    "\n",
    "        return result\n",
    "\n",
    "    def and_not_merge(self, postings1, postings2):\n",
    "        result = []\n",
    "        p1 =0\n",
    "        p2 = 0\n",
    "        while p1 < len(postings1) and p2 < len(postings2):\n",
    "            \n",
    "            p1_doc_id = list(postings1[p1].keys())[0]\n",
    "            p2_doc_id = postings2[p2]\n",
    "            \n",
    "            if p1_doc_id == p2_doc_id:\n",
    "                p1 += 1\n",
    "                p2 += 1\n",
    "            elif p1_doc_id < p2_doc_id:\n",
    "                result.append(postings1[p1])\n",
    "                p1 += 1\n",
    "            else:\n",
    "                p2 += 1\n",
    "\n",
    "        return result\n",
    "\n",
    "    def positional_merge(self, postings1, postings2):\n",
    "        result = []\n",
    "        p1 =0\n",
    "        p2 = 0\n",
    "        while p1 < len(postings1) and p2 < len(postings2):\n",
    "            \n",
    "            p1_doc_id = list(postings1[p1].keys())[0]\n",
    "            p2_doc_id = list(postings2[p2].keys())[0]\n",
    "            \n",
    "            if p1_doc_id == p2_doc_id:\n",
    "                common_pos = []\n",
    "                positions1 = postings1[p1][p1_doc_id]['positions']\n",
    "                positions2 = postings2[p2][p2_doc_id]['positions']\n",
    "                pp1 =0\n",
    "                pp2 = 0\n",
    "                while pp1 < len(positions1) and pp2 < len(positions2):\n",
    "                    # check if the second word is right after first word or not\n",
    "                    if positions1[pp1] + 1 == positions2[pp2]:\n",
    "                        # add position of second word if we want to check the words after that\n",
    "                        # then we can do this \n",
    "                        common_pos.append(positions2[pp2])\n",
    "                        pp1 += 1\n",
    "                        pp2 += 1\n",
    "                    elif positions1[pp1] + 1 < positions2[pp2]:\n",
    "                        pp1 += 1\n",
    "                    else:\n",
    "                        pp2 += 1\n",
    "                if len(common_pos) != 0:\n",
    "                    result.append({p1_doc_id:{\n",
    "                        'term_frequency': len(common_pos),\n",
    "                        'positions':common_pos\n",
    "                    }})\n",
    "                p1 += 1\n",
    "                p2 += 1\n",
    "            elif p1_doc_id < p2_doc_id:\n",
    "                p1 += 1\n",
    "            else:\n",
    "                p2 += 1\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35760dc",
   "metadata": {},
   "source": [
    "# Step5)\n",
    "## Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154018f2",
   "metadata": {},
   "source": [
    "**Q1:** Describe with an example what operation you did in the pre-processing step. Also mention the reason for each processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e2e0e4",
   "metadata": {},
   "source": [
    "**Answer**: \n",
    "- The details related to this section would be seen in Step1) Preprocessing section. we use below steps:\n",
    "    - [ ] **Removing punctuation marks** : because it reduce our dectionary size and also we shouldn't count them as tokens\n",
    "    - [ ] **Normalization** : In this step actions like: change all numbers in Eng. language, delete extra spaces and ... are done.\n",
    "    - [ ] **Tokenizing** : The output of prevoius step now can be splited in tokens. We use tokens in dictionary. also we call them terms. \n",
    "    - [ ] **Removing Stopwords**: Some words in every languages are very frequent like \"The\" or \"را\" in Persian. They dont give us any information to handle queries and If we consider them, it will lead to wrong results. \n",
    "    - [ ] **Stemming** : change every words to their roots. Because we don't like words with the same root and different appearance to be recognized differently from one another, and we also don't want each one to be an independent member in the index. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a203a16",
   "metadata": {},
   "source": [
    "**Q2:** Check the validity of Zipf's law in two situations before and after removing stop words from the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00599609",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "- We have below formula for this law:\n",
    "$$\\log cf_i = \\log K - \\log i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c072689",
   "metadata": {},
   "source": [
    "below function get preprocessed content and give ust the cf of each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfc52cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cf(preprocessed_contents):\n",
    "    token_cf = {}\n",
    "    for content in preprocessed_contents:\n",
    "        for token in content:\n",
    "            if token_cf.get(token):\n",
    "                token_cf[token] += 1\n",
    "            else:\n",
    "                token_cf[token] = 1\n",
    "    cf = [token_cf[token] for token in token_cf]\n",
    "    cf.sort(reverse=True)\n",
    "    return cf "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d9365b",
   "metadata": {},
   "source": [
    "- [ ] **With cosidering Stopwords:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c038864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1) find preprocess contents and also considering stopwords\n",
    "preprocessed_with_stopwords = []\n",
    "for i in range(len(df)):\n",
    "    if ( i % 2000 ==0):\n",
    "        print(i)\n",
    "    input_content_zipf_with_stop = df.iloc[i]['content']\n",
    "    new_content = total_preprocess (input_content_zipf_with_stop, is_stopwords_remove=False)\n",
    "    preprocessed_with_stopwords.append(new_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b4c952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2) find cf for tokens\n",
    "cf = get_cf(preprocessed_with_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4869d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step3) define horizontal axis or log_rank variable and vertical axis or log_cf varibale\n",
    "log_rank = list(map(math.log, range(1, len(cf)+1)))\n",
    "log_cf = [math.log(single_cf) for single_cf in cf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1494b768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step4) plot\n",
    "plt.title('Zipf Graph')\n",
    "plt.xlabel('log Rank')\n",
    "plt.ylabel('log cf')\n",
    "plt.plot(log_rank, log_cf, label='With Stopwords')\n",
    "plt.plot(range(math.ceil(max(log_cf))), range(math.ceil(max(log_cf)))[::-1], label='Ideal Graph')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416835eb",
   "metadata": {},
   "source": [
    "- [ ] **Removing Stopwords:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f0b1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1) find preprocessed contents without considering stopwords\n",
    "# (We do it before.in Default mode, the total_preprocess function remove stopwords)\n",
    "\n",
    "# Step2) find cf for tokens\n",
    "# here we can calculate summation of tf for a token or use last method\n",
    "cf2 = get_cf(preprocessed_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb18bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step3) define horizontal axis or log_rank variable and vertical axis or log_cf varibale\n",
    "log_rank2 = list(map(math.log, range(1, len(cf2)+1)))\n",
    "log_cf2 = [math.log(single_cf) for single_cf in cf2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5d386d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step4) plot\n",
    "plt.title('Zipf Graph')\n",
    "plt.xlabel('log Rank')\n",
    "plt.ylabel('log cf')\n",
    "plt.plot(log_rank2, log_cf2, label='Without Stopwords')\n",
    "plt.plot(range(math.ceil(max(log_cf))), range(math.ceil(max(log_cf)))[::-1], label='Ideal Graph')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f0eb16",
   "metadata": {},
   "source": [
    "**Q3:** Check the correctness of heaps law in two cases before and after stemming. To check this law, it is necessary to estimate the size of the dictionary for all documents using the size of the dictionary and the number of tokens in the first 500, 1000, 1500 and 2000 documents. Finally, the actual size of the dictionary and the estimated size in both cases should be compared and analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b35cdf",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "- We have below formula for this law:\n",
    "$$\\log M = \\log K + b\\log T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac49870",
   "metadata": {},
   "source": [
    "with the help of below function we can find T and M in above formula according to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c99897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_T_M(preprocessed_contents):\n",
    "    # we define the dictionary as a set. because we send tokens inside it and \n",
    "    # just we want number of different terms( We know terms in dictionary are keys and also they are unique)\n",
    "    dictionary = set()\n",
    "    actual_dictionary_size = []\n",
    "    \n",
    "    visited_docs_num = 0\n",
    "    \n",
    "    info_for_estimation = {}\n",
    "    \n",
    "    for content in preprocessed_contents:\n",
    "        for token in content:\n",
    "            dictionary.add(token)\n",
    "            actual_dictionary_size.append(len(dictionary))\n",
    "        visited_docs_num += 1\n",
    "        \n",
    "        if visited_docs_num in [500,1000,1500,2000]:\n",
    "            info_for_estimation[visited_docs_num] ={'M':len(dictionary), 'T':len(actual_dictionary_size)}\n",
    "    return actual_dictionary_size, info_for_estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec86ff21",
   "metadata": {},
   "source": [
    "for prediction we use simple linear reqression model. first we should train it and then use it for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36acdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(T_list, M_list):\n",
    "    model = LinearRegression()\n",
    "    x = np.array(T_list).reshape(-1, 1)\n",
    "    y = np.array(M_list)\n",
    "    model.fit(x, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5e2c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters(actual_dictionary_size, info_for_estimation):\n",
    "    T_list_for_prediction = []\n",
    "    M_list_for_prediction = []\n",
    "    for doc_nums in info_for_estimation:\n",
    "        M_list_for_prediction.append(math.log(info_for_estimation[doc_nums]['M']))\n",
    "        T_list_for_prediction.append(math.log(info_for_estimation[doc_nums]['T']))\n",
    "\n",
    "    model = get_model(T_list_for_prediction, M_list_for_prediction)\n",
    "    T = [math.log(i) for i in range(1, len(actual_dictionary_size)+1)]\n",
    "    M_actual = [math.log(i) for i in actual_dictionary_size]\n",
    "    T_predicted = [0] + T_list_for_prediction + [math.log(len(actual_dictionary_size))]\n",
    "    M_predicted = model.predict(np.array(T_predicted).reshape(-1,1)).tolist()\n",
    "    \n",
    "    return T_list_for_prediction, M_list_for_prediction, T, M_actual, M_predicted, T_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b3b2a3",
   "metadata": {},
   "source": [
    "- [ ] **Without Stemming:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be499c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1) find preprocess contents and also stemming isnt done\n",
    "preprocessed_without_stemming = []\n",
    "for i in range(len(df)):\n",
    "    if ( i % 2000 ==0):\n",
    "        print(i)\n",
    "    input_content_heaps_without_stemming = df.iloc[i]['content']\n",
    "    new_content = total_preprocess (input_content_heaps_without_stemming, is_stemming=False)\n",
    "    preprocessed_without_stemming.append(new_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4c5c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2) get T and M for each of tokens\n",
    "actual_dictionary_size, info_for_estimation = get_T_M(preprocessed_without_stemming)\n",
    "# Step3) find the data of variables that we want to plot\n",
    "T_list_for_prediction, M_list_for_prediction, T, M_actual, M_predicted, T_predicted = get_parameters(actual_dictionary_size, info_for_estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf98a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Heaps law Graph (without stemming)')\n",
    "plt.xlabel('log T')\n",
    "plt.ylabel('log M')\n",
    "plt.plot(T, M_actual, label='actual Vocabulary size')\n",
    "plt.plot(T_predicted, M_predicted, linestyle='--', label='pridicted vocabulary size')\n",
    "plt.plot(T_list_for_prediction, M_list_for_prediction, 'ro', label='Sampled points')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3243b858",
   "metadata": {},
   "source": [
    "- [ ] **With Stemming:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4670aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2) get T and M for each of tokens\n",
    "actual_dictionary_size, info_for_estimation = get_T_M(preprocessed_content)\n",
    "# Step3) find the data of variables that we want to plot\n",
    "T_list_for_prediction, M_list_for_prediction, T, M_actual, M_predicted, T_predicted = get_parameters(actual_dictionary_size, info_for_estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa386b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Heaps law Graph')\n",
    "plt.xlabel('log T')\n",
    "plt.ylabel('log M')\n",
    "plt.plot(T, M_actual, label='actual Vocabulary size')\n",
    "plt.plot(T_predicted, M_predicted, linestyle='--', label='pridicted vocabulary size')\n",
    "plt.plot(T_list_for_prediction, M_list_for_prediction, 'ro', label='Sampled points')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e97c678",
   "metadata": {},
   "source": [
    "**Q4:** Mention at least three cases where you faced challenges in Stemming. (For example, words that do not need stemming, but are lost according to the process of stemming.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03a9576",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "- Some of roots are not correct. forexample the root of \"هسته\" isn't \"هست\"\n",
    "- In some situations we see some meaningless words or with %u sign and somthing like these\n",
    "- For some words a list of roots is returned.\n",
    "- Some plural words specificly words with \"ات\" are stemmed incorrectly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e13f4b",
   "metadata": {},
   "source": [
    "**Q5:** Answer the question in the following modes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0afad1",
   "metadata": {},
   "source": [
    "- [ ] A query of simple and frequently used words (such as 'تحریم‌های آمریکا علیه ایران', in the retrieved results it is expected that documents that contain the words sanctions, آمریکا, علیه and ایران are at the top of the list and documents that do not have some words at the lower ranks of the list are supposed to be)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798e3fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Query('تحریم‌های آمریکا علیه ایران')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b4f073",
   "metadata": {},
   "source": [
    "- [ ] A query with the NOT operator (such as 'تحریم‌های آمریکا ! ایران', it is expected that documents containing the two words تحریم and امریکا, but not the word ایران, will be present in the retrieved results.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a32e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "Query('تحریم‌های آمریکا ! ایران')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ce4a56",
   "metadata": {},
   "source": [
    "- [ ] A query with a phrase operator (such as \"کنگره ضدتروریست\", it is expected that there will be documents containing the phrase کنگره ضدتروریست in the retrieved results; in other words, the location of the words is important in this case.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a2205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Query('\"کنگره ضدتروریست\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bdde9b",
   "metadata": {},
   "source": [
    "- [ ] A complex query (like 'تحریم هسته‌ای\" آمریکا ! ایران\"', it is expected that there will be documents that include the word تحریم هسته‌ای and the word آمریکا but not the word ایران in the retrieved results.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499b8105",
   "metadata": {},
   "outputs": [],
   "source": [
    "Query('\"تحریم هسته‌ای\" آمریکا گزارش ! ایران')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be665a3e",
   "metadata": {},
   "source": [
    "- [ ] A query of rare words (such as 'اورشلیم ! صهیونیست', the expected output of this part is similar to part B, with the difference that the words used in the query are rare words.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8112c532",
   "metadata": {},
   "outputs": [],
   "source": [
    "Query('اورشلیم ! صهیونیست')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
