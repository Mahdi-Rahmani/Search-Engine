{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9f344a9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h1 align=\"center\">Information Retrival Systems</h1>\n",
    "    <h2 align=\"center\">Simple search engine project</h2>\n",
    "    <h3 align=\"center\">Phase 2</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89e9de6",
   "metadata": {},
   "source": [
    "## Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9cb27e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsivar import Normalizer, Tokenizer, FindStems\n",
    "from hazm import stopwords_list\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aca712",
   "metadata": {},
   "source": [
    "## Loading raw and processed Data\n",
    "- loading the json file as a dataframe and also transpose it\n",
    "- loading preprocessed content\n",
    "- loading inverted index\n",
    "- create dictionary according to inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f91f7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('IR_data_news_12k.json')\n",
    "df = df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58e571e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filename):\n",
    "    my_file = None\n",
    "    with open(filename, 'rb') as inp:\n",
    "        my_file = pickle.load(inp)\n",
    "    return my_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4f7b439",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_content = load_file('./Processed data/preprocessed_content.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d125d683",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = load_file('./Processed data/inverted_index.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00528117",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = list(inverted_index.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fbf89c",
   "metadata": {},
   "source": [
    "# Step1)\n",
    "## Modeling documents in vector space\n",
    "- Here we should define a function that calculate tf_idf score. The formula explained below:\n",
    "$$tfidf(t, d, D) = tf(t, d) \\times idf(t, D) = \\ (1 + log(f_{t, d})) \\times \\log(\\frac{N}{n_t})$$    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dd7bacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(term, doc, doc_collection, total_number_of_docs):\n",
    "    # idf is a constant value for each token and it doesnt depend on query\n",
    "    N = total_number_of_docs\n",
    "    nt = doc_collection[term]['doc_frequency']\n",
    "    idf = math.log10(N / nt)\n",
    "    \n",
    "    # now we should compute number of term in that doc\n",
    "    term_count = doc.count(term)\n",
    "    if term_count == 0:\n",
    "        tf = 0\n",
    "    else:\n",
    "        tf = 1+math.log10(term_count)\n",
    "    \n",
    "    return  tf*idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc09cb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007249774608743844\n"
     ]
    }
   ],
   "source": [
    "print(tfidf('خبرگزاری',df.iloc[3]['content'], inverted_index , len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07152e92",
   "metadata": {},
   "source": [
    "# Step2)\n",
    "## Answering the queries in the vector space  \n",
    "\n",
    "- Here we should define a function for calculating cosine similarity. Also we define a function to compute Doc_length.\n",
    "- For this part we help from pseudo code in slides.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "571ae640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_length(doc_id):\n",
    "    l = 0\n",
    "    for token in preprocessed_content[doc_id]:\n",
    "        l += tfidf(term=token, doc=preprocessed_content[doc_id], doc_collection=inverted_index, total_number_of_docs=len(df))**2\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c83684fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(query, k, inverted_index, is_champion=True):\n",
    "    docID_score = {}\n",
    "    doc_ids = []\n",
    "    for term in query:\n",
    "        # step 1: calculate weight of term in the query\n",
    "        weight_in_query = tfidf(term=term, doc=query, doc_collection=inverted_index, total_number_of_docs=len(df))\n",
    "        # step 2: get postings list for term (if is_champion is true we should use ch)\n",
    "        if is_champion:\n",
    "            postings_list = inverted_index[term]['champion']\n",
    "        else:\n",
    "            postings_list = inverted_index[term]['posting_list']\n",
    "            \n",
    "        for posting in postings_list:\n",
    "            if is_champion:\n",
    "                doc_id = posting[0]\n",
    "            else:\n",
    "                doc_id = list(posting.keys())[0]\n",
    "            # step 3: calculate weight of term in the Doc\n",
    "            weight_in_doc = tfidf(term=term, doc=df.iloc[doc_id]['content'],  doc_collection=inverted_index, total_number_of_docs=len(df))\n",
    "            \n",
    "            # step4:save or update related score of a doc\n",
    "            if docID_score.get(doc_id):\n",
    "                docID_score[doc_id]['score'] += weight_in_query * weight_in_doc\n",
    "                #docID_score[doc_id]['squar length'] += (weight_in_doc ** 2)\n",
    "            else:\n",
    "                doc_ids.append(doc_id)\n",
    "                #docID_score[doc_id]= {'squar length': weight_in_doc ** 2, 'score':weight_in_query * weight_in_doc}\n",
    "                docID_score[doc_id]= {'squar length': get_length(doc_id), 'score':weight_in_query * weight_in_doc}\n",
    "    # step5: normalizing\n",
    "    new_docID_score = []\n",
    "    for doc_id in doc_ids:\n",
    "        docID_score[doc_id]['score'] = docID_score[doc_id]['score']/math.sqrt(docID_score[doc_id]['squar length'])\n",
    "        new_docID_score.append((doc_id,docID_score[doc_id]['score']))\n",
    "    # step6: select k best:\n",
    "    new_docID_score = [(docID_score[0],docID_score[1]['score']) for docID_score in docID_score.items()]\n",
    "    if k > len(new_docID_score):\n",
    "        k = len(new_docID_score)\n",
    "    k_best = []\n",
    "    for i in range(k):\n",
    "        max_score = max(new_docID_score,key=lambda x:x[1])\n",
    "        k_best.append(max_score)\n",
    "        new_docID_score.remove(max_score)\n",
    "    return k_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a033de83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
